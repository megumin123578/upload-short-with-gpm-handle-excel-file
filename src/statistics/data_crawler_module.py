import requests, re
import requests
from bs4 import BeautifulSoup
import csv
import os
import pandas as pd

CSV_PATH = r"statistics\data\orders_all.csv"
CLEAN_CSV_PATH = r"statistics\data\orders_clean.csv"
session = requests.Session()
cookies = {
    "PHPSESSID": "hp4055kjcj3po9pon4qv5sonlt",
}
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36",
    "Accept-Language": "en-US,en;q=0.9",
    "Referer": "https://smmstore.pro/api",
}
#read older csv
existing_ids = set()

API_KEY = "AIzaSyDwWltIR-XP9V61V7RNTTz-G04mVfHKlsQ"

def url_to_channel(url: str):
    # L·∫•y video_id t·ª´ URL
    match = re.search(r"(?:v=|youtu\.be/)([A-Za-z0-9_-]{11})", url)
    if not match:
        return None
    video_id = match.group(1)

    api_url = f"https://www.googleapis.com/youtube/v3/videos?part=snippet&id={video_id}&key={API_KEY}"
    r = requests.get(api_url)
    data = r.json()

    if "items" in data and data["items"]:
        return data["items"][0]["snippet"]["channelTitle"]
    else:
        return "Unidentified"


def crawl_data(csv_path=CSV_PATH):
    if os.path.exists(csv_path):
        with open(csv_path, newline="", encoding="utf-8") as f:
            reader = csv.reader(f)
            for row in reader:
                if row:
                    existing_ids.add(row[0].strip())
        print(f"ƒê√£ t·∫£i {len(existing_ids)} ID c≈© t·ª´ {csv_path}")

    all_rows = []
    page = 1
    stop_flag = False

    while not stop_flag:
        url = f"https://smmstore.pro/orders?page={page}"
        print("Crawling:", url)
        r = session.get(url, headers=headers, cookies=cookies)
        soup = BeautifulSoup(r.text, "html.parser")
        table = soup.find("table")

        if not table:
            print("Kh√¥ng t√¨m th·∫•y b·∫£ng ‚Äî c√≥ th·ªÉ h·∫øt trang ho·∫∑c cookie h·∫øt h·∫°n.")
            break

        rows = table.find_all("tr")[1:]
        num_rows = len(rows)
        if num_rows == 0:
            print(f"H·∫øt d·ªØ li·ªáu sau trang {page}")
            break

        for tr in rows:
            cols = [td.get_text(strip=True) for td in tr.find_all("td")]
            if not cols:
                continue

            # ‚úÖ B·ªè c·ªôt r·ªóng ·ªü cu·ªëi (ngƒÉn sinh d·∫•u ph·∫©y th·ª´a)
            while cols and cols[-1] == "":
                cols.pop()

            order_id = cols[0]
            if order_id in existing_ids:
                print(f"Ph√°t hi·ªán ID {order_id} ƒë√£ t·ªìn t·∫°i trong CSV ‚Äî d·ª´ng crawler.")
                stop_flag = True
                break

            existing_ids.add(order_id)
            all_rows.append(cols)

        if stop_flag:
            break

        print(f"L·∫•y ƒë∆∞·ª£c {num_rows} h√†ng t·ª´ trang {page}")

        if num_rows < 100:
            print(f"H·∫øt d·ªØ li·ªáu ·ªü trang {page} (ch·ªâ c√≥ {num_rows} h√†ng)")
            break

        page += 1

    if all_rows:
        headers_list = ["ID", "Date", "Link", "Charge", "Start count",
                        "Quantity", "Service", "Status", "Remains"]

        file_exists = os.path.exists(csv_path)
        with open(csv_path, "a", newline="", encoding="utf-8") as f:
            writer = csv.writer(f)
            if not file_exists or os.stat(csv_path).st_size == 0:
                writer.writerow(headers_list)
            writer.writerows(all_rows)

        print(f"ƒê√£ ghi th√™m {len(all_rows)} ƒë∆°n h√†ng m·ªõi v√†o {csv_path}")
    else:
        print("Kh√¥ng c√≥ ƒë∆°n m·ªõi ƒë·ªÉ ghi.")

    print(f"T·ªïng c·ªông hi·ªán c√≥ {len(existing_ids)} ƒë∆°n h√†ng trong file CSV.")



def pre_process_data(file):
    df = pd.read_csv(file, index_col=False, sep=None, engine='python')
    filtered_df = df[["Date", "Link", "Charge"]].dropna(subset=["Date", "Link", "Charge"])
    return filtered_df


def clean_data(input_path = "statistics\data\orders_all.csv", output_path="statistics\data\orders_clean.csv", expected_cols = 9 ):
    with open(input_path, encoding="utf-8-sig") as infile, \
        open(output_path, "w", encoding="utf-8-sig", newline="") as outfile:

        reader = csv.reader(infile)
        writer = csv.writer(outfile)

        for i, row in enumerate(reader, start=1):
            if len(row) == expected_cols:
                writer.writerow(row)
            elif len(row) > expected_cols:
                # keep 9 cols
                writer.writerow(row[:expected_cols])
                print(f"D√≤ng {i}: c√≥ {len(row)} c·ªôt, ƒë√£ c·∫Øt b·ªõt c√≤n {expected_cols}.")
            else:
                # skip
                print(f"D√≤ng {i}: ch·ªâ c√≥ {len(row)} c·ªôt, ƒë√£ b·ªè qua.")

    print(f"ƒê√£ l√†m s·∫°ch file, l∆∞u t·∫°i: {output_path}")

import pandas as pd
from concurrent.futures import ThreadPoolExecutor, as_completed

def replace_link_with_channel(df, month, max_workers=10):
    df = df.copy()
    output_file = f"statistics/data/orders_with_channels_{month}.csv"

    # === 1. X·ª≠ l√Ω c·ªôt ng√†y ===
    date_col = next((c for c in df.columns if 'date' in c.lower()), None)
    if not date_col:
        raise KeyError("Kh√¥ng t√¨m th·∫•y c·ªôt ch·ª©a 'date' trong CSV!")
    df[date_col] = pd.to_datetime(df[date_col], errors="coerce")
    df = df[df[date_col].dt.strftime("%Y-%m") == month]

    # === 2. L·∫•y danh s√°ch URL duy nh·∫•t ===
    unique_links = df["Link"].dropna().unique().tolist()
    print(f"üîó C√≥ {len(unique_links)} ƒë∆∞·ªùng d·∫´n c·∫ßn x·ª≠ l√Ω...")

    cache = {}

    def fetch_channel(url):
        """G·ªçi url_to_channel 1 l·∫ßn, c√≥ ki·ªÉm tra cache"""
        if url in cache:
            return url, cache[url]
        try:
            channel = url_to_channel(url)
            cache[url] = channel
            return url, channel
        except Exception as e:
            print(f"L·ªói khi x·ª≠ l√Ω {url}: {e}")
            return url, None

    # === 3. D√πng ThreadPool ƒë·ªÉ ch·∫°y song song ===
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(fetch_channel, url): url for url in unique_links}
        for i, future in enumerate(as_completed(futures), 1):
            url, channel = future.result()
            print(f"[{i}/{len(unique_links)}] {url} ‚Üí {channel}")

    # === 4. G√°n k·∫øt qu·∫£ l·∫°i cho DataFrame ===
    df["Link"] = df["Link"].map(cache).fillna(df["Link"])

    df.to_csv(output_file, index=False, encoding="utf-8")
    print(f"ƒê√£ x·ª≠ l√Ω {len(df)} d√≤ng trong th√°ng {month}, l∆∞u t·∫°i: {output_file}")
    return df
